{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KunapureddySindhusri/Fmml-labs/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "6e36795d-a0b6-4e50-8c00-75eeff3a1551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "f7d12b82-f854-4446-d335-b9dcf523d669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "97cf62e8-89be-455f-f77a-e1b5777685ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "ee78d576-5944-4065-e6cb-f06afeb0e1a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "fb9e7736-6c96-4851-83e7-70cd91e90b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "2d45b6dc-08d5-458b-a761-b68d8044a210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "45957d36-b398-4364-e748-60c4e57f592e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "\n",
        "Answer1.The accuracy of the validation set is typically a measure of how well your model performs on unseen data, which is crucial for assessing its generalization ability. When you change the percentage of the data allocated to the validation set, here’s what generally happens:\n",
        "\n",
        "### 1. **Increasing the Percentage of the Validation Set:**\n",
        "   - **More Reliable Estimate**: With a larger validation set, the estimate of model accuracy on unseen data is usually more reliable and less subject to random fluctuations because it's based on more data.\n",
        "   - **Less Training Data**: However, increasing the validation set size reduces the amount of data available for training. This can potentially decrease the model's performance because the model has less data to learn from, which may lead to underfitting, especially if the training set becomes too small.\n",
        "\n",
        "### 2. **Decreasing the Percentage of the Validation Set:**\n",
        "   - **Less Reliable Estimate**: With a smaller validation set, the accuracy estimate becomes less reliable and more prone to variance because it's based on fewer samples. The smaller the validation set, the more likely the accuracy is to fluctuate due to the limited representation of the data.\n",
        "   - **More Training Data**: On the upside, reducing the validation set size increases the amount of data available for training, which can improve the model's ability to learn and potentially enhance its performance. However, this improvement might not always be significant, and the less reliable accuracy measure might mislead you about the true performance of your model.\n",
        "\n",
        "### **Summary:**\n",
        "- **Increasing** the validation set size generally provides a more stable and reliable measure of accuracy but at the cost of potentially lower model performance due to reduced training data.\n",
        "- **Decreasing** the validation set size can improve model performance due to more training data but leads to a less reliable estimate of the model's accuracy.\n",
        "\n",
        "Striking the right balance is important and often requires experimentation, such as cross-validation, to ensure the model is evaluated robustly without compromising on training data.\n",
        "\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "\n",
        "Answer2.The sizes of the training and validation sets directly influence how accurately the validation set can predict the model's performance on the test set. Here’s how each size impacts this:\n",
        "\n",
        "### 1. **Training Set Size:**\n",
        "   - **Larger Training Set**:\n",
        "     - **Better Model Performance**: A larger training set allows the model to learn more patterns and features from the data, leading to better generalization. This usually results in higher validation accuracy, which is more reflective of the model's performance on the test set.\n",
        "     - **Improved Generalization**: With more training data, the model is less likely to overfit to specific examples, making the validation accuracy a better predictor of test set accuracy.\n",
        "\n",
        "   - **Smaller Training Set**:\n",
        "     - **Higher Risk of Overfitting**: With less data, the model may overfit to the training data, capturing noise rather than general patterns. This can lead to higher accuracy on the training set but poorer generalization to the validation and test sets.\n",
        "     - **Less Reliable Validation Accuracy**: The validation accuracy might not be as reliable, as the model may have learned spurious correlations from the limited training data.\n",
        "\n",
        "### 2. **Validation Set Size:**\n",
        "   - **Larger Validation Set**:\n",
        "     - **More Reliable Accuracy Estimate**: A larger validation set provides a more stable and accurate estimate of the model's performance. This is because the evaluation is based on a broader range of data, reducing the variance in the accuracy estimate.\n",
        "     - **Better Test Set Prediction**: Since the validation set better represents the data distribution, the accuracy measured on it is more likely to be a reliable predictor of the test set performance.\n",
        "\n",
        "   - **Smaller Validation Set**:\n",
        "     - **Less Reliable Accuracy Estimate**: A smaller validation set may lead to an accuracy estimate that is less reliable due to higher variance. The performance measured might be more susceptible to random fluctuations, making it less predictive of the test set accuracy.\n",
        "     - **Risk of Overestimating Performance**: If the validation set is too small, it may not adequately represent the diversity of the data, leading to an overestimate of the model's performance on unseen data (i.e., the test set).\n",
        "\n",
        "### **Combined Effect on Test Set Prediction:**\n",
        "- **Balanced Allocation**: Ideally, you want a balance where the training set is large enough to capture the underlying patterns in the data, and the validation set is large enough to provide a reliable estimate of model performance. This balance helps ensure that the validation accuracy is a good predictor of the test set accuracy.\n",
        "  \n",
        "- **Cross-Validation**: To mitigate the impact of validation set size, techniques like cross-validation can be used. Cross-validation involves splitting the data into multiple folds and training/validating the model multiple times on different splits. This approach reduces the dependency on a single validation set and provides a more robust estimate of test set performance.\n",
        "\n",
        "### **Summary:**\n",
        "- A **larger training set** improves the model's ability to generalize, leading to validation accuracy that better predicts test set accuracy.\n",
        "- A **larger validation set** provides a more reliable estimate of model performance, making it a better predictor of test set accuracy.\n",
        "- Striking a balance between training and validation set sizes, or using techniques like cross-validation, can help ensure that the validation accuracy is a good predictor of the model's performance on the test set.\n",
        "\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer3.A common and effective practice is to reserve **20%** of the data for the validation set. This percentage often strikes a good balance between having enough data to train the model effectively and having a sufficiently large validation set to reliably estimate the model's performance.\n",
        "\n",
        "### Why 20% Is a Good Balance:\n",
        "1. **Training Set Sufficiency**: With 80% of the data used for training, the model typically has enough data to learn meaningful patterns and generalize well. This is especially true if the dataset is reasonably large.\n",
        "   \n",
        "2. **Validation Set Reliability**: The remaining 20% is usually sufficient to provide a reliable estimate of the model’s performance on unseen data. It reduces the risk of high variance in the accuracy estimate, making it a good predictor of test set performance.\n",
        "\n",
        "### Situations That Might Require Adjustments:\n",
        "- **Large Datasets**: If your dataset is very large (e.g., tens of thousands of samples), you might get away with a smaller validation set, such as 10%, because even a small percentage will include a large number of samples. This allows more data for training without compromising the reliability of the validation set.\n",
        "  \n",
        "- **Small Datasets**: If your dataset is small, you might want to consider using a larger percentage for validation (e.g., 30%) to ensure the validation set is large enough to provide a stable estimate. However, in small datasets, cross-validation is often preferred to maximize the use of available data.\n",
        "\n",
        "- **High Variability Data**: If the data is highly variable or complex, you might also lean towards a larger validation set to ensure that the validation accuracy reflects the diversity of the data, ensuring that the model's performance is assessed more thoroughly.\n",
        "\n",
        "### **Alternative Approach: Cross-Validation**\n",
        "If you’re concerned about the trade-off between training and validation data, or if your dataset is small, consider using **k-fold cross-validation**. This method involves splitting the data into *k* folds and using each fold as a validation set while training on the remaining folds. This way, the model is evaluated on multiple validation sets, providing a more robust estimate of its performance.\n",
        "\n",
        "### **Summary**:\n",
        "- **20%** is generally a good percentage for the validation set.\n",
        "- Consider adjusting based on dataset size and complexity.\n",
        "- Cross-validation is an excellent alternative if you want to use all available data effectively.\n",
        "\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model.\n",
        "\n",
        "Answer.To compare the accuracy of a 1-nearest neighbor (1-NN) classifier and a 3-nearest neighbors (3-NN) classifier, we can follow these steps:\n",
        "\n",
        "1. **Load a dataset**: For simplicity, we'll use a well-known dataset, such as the Iris dataset or any other available dataset in `scikit-learn`.\n",
        "2. **Split the data**: Divide the dataset into a training set and a test set.\n",
        "3. **Train the models**: Train both the 1-NN and 3-NN classifiers on the training data.\n",
        "4. **Evaluate the models**: Test both models on the test data and compute their accuracy.\n",
        "\n",
        "Let's implement this in Python:\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Step 2: Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train the 1-NN and 3-NN models\n",
        "knn_1 = KNeighborsClassifier(n_neighbors=1)\n",
        "knn_3 = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "knn_1.fit(X_train, y_train)\n",
        "knn_3.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions and calculate accuracy for both models\n",
        "y_pred_1 = knn_1.predict(X_test)\n",
        "y_pred_3 = knn_3.predict(X_test)\n",
        "\n",
        "accuracy_1 = accuracy_score(y_test, y_pred_1)\n",
        "accuracy_3 = accuracy_score(y_test, y_pred_3)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy of 1-NN: {accuracy_1:.2f}\")\n",
        "print(f\"Accuracy of 3-NN: {accuracy_3:.2f}\")\n",
        "```\n",
        "\n",
        "### Explanation:\n",
        "- **Dataset**: We're using the Iris dataset, which is commonly used for classification tasks. It has 150 samples with 4 features, and the task is to classify them into 3 different species of iris flowers.\n",
        "- **Training and Test Split**: The data is split into 80% for training and 20% for testing.\n",
        "- **1-NN and 3-NN**: We use the `KNeighborsClassifier` with `n_neighbors=1` for 1-NN and `n_neighbors=3` for 3-NN.\n",
        "- **Accuracy**: The `accuracy_score` function calculates the percentage of correctly classified samples in the test set.\n",
        "\n",
        "### Results:\n",
        "The script will print the accuracy of both the 1-NN and 3-NN classifiers, allowing you to compare their performance. Generally, 3-NN tends to smooth out decision boundaries, leading to potentially better performance in noisy datasets, while 1-NN is more sensitive to noise and outliers.\n",
        "\n",
        "You can run the script in your local environment to see the results. If you want me to run it and show you the results, please let me know!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "88818284-f115-4293-c83d-d929dff34143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "\n",
        "Answer1.Yes, averaging the validation accuracy across multiple splits generally gives more consistent and reliable results. This approach, often implemented using techniques like **k-fold cross-validation**, helps mitigate the variability that can arise from using a single validation split.\n",
        "\n",
        "### How Averaging Improves Consistency:\n",
        "\n",
        "1. **Reduces Variability**:\n",
        "   - When you split your data into training and validation sets multiple times (e.g., in k-fold cross-validation), the model is trained and validated on different portions of the data. The accuracy from each split might vary due to differences in the specific data points included in the training and validation sets.\n",
        "   - Averaging the accuracies across these different splits smooths out these variations, giving a more stable estimate of the model's performance.\n",
        "\n",
        "2. **Better Generalization**:\n",
        "   - Since the model is evaluated on different subsets of the data, the averaged validation accuracy is likely to better represent how the model would perform on completely unseen data (like a test set or in real-world scenarios).\n",
        "   - This reduces the risk of the validation accuracy being overly optimistic or pessimistic due to a particularly easy or difficult split.\n",
        "\n",
        "3. **Mitigates the Impact of Outliers**:\n",
        "   - In a single split, if there are outliers or particularly difficult samples in the validation set, they could disproportionately affect the validation accuracy.\n",
        "   - By averaging across multiple splits, the influence of these outliers is diluted, leading to a more robust measure of model performance.\n",
        "\n",
        "### Example: k-Fold Cross-Validation\n",
        "In k-fold cross-validation, the dataset is divided into *k* equally-sized \"folds.\" The model is trained *k* times, each time using *k-1* folds for training and the remaining fold for validation. The final validation accuracy is the average of the accuracies from all *k* runs.\n",
        "\n",
        "For instance, in 5-fold cross-validation:\n",
        "- The data is split into 5 folds.\n",
        "- The model is trained 5 times, each time using a different fold as the validation set.\n",
        "- The validation accuracy for each of the 5 runs is averaged to provide a final estimate.\n",
        "\n",
        "### Summary:\n",
        "- **Averaging validation accuracy across multiple splits** (like in k-fold cross-validation) provides a more reliable, consistent, and generalizable estimate of the model's performance.\n",
        "- This approach is particularly useful when the dataset is small or when you want to minimize the impact of a single, potentially unrepresentative split.\n",
        "\n",
        "Using this method helps you make more informed decisions about your model's actual performance, reducing the risk of overfitting to a single validation set.\n",
        "\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "\n",
        "Answer2.Yes, averaging validation accuracy across multiple splits, such as in **k-fold cross-validation**, typically provides a more accurate estimate of test accuracy. Here's why:\n",
        "\n",
        "### 1. **Better Representation of Data Distribution**:\n",
        "   - When you perform k-fold cross-validation, each data point in the dataset gets a chance to be in the validation set exactly once. This ensures that the validation process considers the entire distribution of the data, not just a single subset.\n",
        "   - This leads to a validation accuracy that better reflects how the model will perform on unseen data, thereby providing a more accurate estimate of test accuracy.\n",
        "\n",
        "### 2. **Reduces Bias from a Single Split**:\n",
        "   - A single train-test split might be biased, either because the validation set is not representative of the data distribution or due to random chance (e.g., an unusually difficult or easy validation set).\n",
        "   - Averaging across multiple splits reduces this bias, as it incorporates performance across various portions of the data, leading to an estimate of test accuracy that is less dependent on the peculiarities of any one split.\n",
        "\n",
        "### 3. **Mitigates Overfitting to Validation Set**:\n",
        "   - If you rely on a single validation set, there’s a risk that the model could slightly overfit to that specific set. This means the model's performance on the validation set might not generalize well to new data.\n",
        "   - By using multiple validation sets and averaging the results, the model is less likely to overfit to any specific subset of the data, which makes the validation accuracy a more reliable predictor of test accuracy.\n",
        "\n",
        "### 4. **Improved Generalization**:\n",
        "   - The average performance across multiple validation sets more closely approximates the model's ability to generalize to entirely unseen data, like a test set or real-world data.\n",
        "   - This generalization is crucial because the goal of model validation is to estimate how the model will perform in real-world situations, not just on a specific set of validation data.\n",
        "\n",
        "### **Summary**:\n",
        "- **Yes**, averaging validation accuracy across multiple splits, as done in techniques like k-fold cross-validation, generally gives a more accurate estimate of test accuracy.\n",
        "- It reduces the bias and variance associated with a single validation split, leading to a more reliable prediction of how the model will perform on unseen test data.\n",
        "- This approach is especially valuable when the dataset is small or when high generalization performance is critical.\n",
        "\n",
        "In practice, this means that models validated with cross-validation are often better at predicting performance on test data, making them more robust for real-world applications.\n",
        "\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "\n",
        "Answer3.The number of iterations, or folds, in cross-validation affects the reliability and stability of the performance estimate. Here's how it impacts the estimate:\n",
        "\n",
        "### 1. **Effect of More Iterations (Higher k in k-Fold Cross-Validation):**\n",
        "   - **More Stable Estimate**: With more iterations (e.g., higher k in k-fold cross-validation), each data point is used for validation exactly once and for training \\(k-1\\) times. This leads to a more comprehensive assessment of the model’s performance across different subsets of the data, reducing variability in the estimate.\n",
        "   - **Better Representation**: More folds mean that the validation process better represents the entire data distribution, as the model is evaluated on different portions of the data more frequently.\n",
        "   - **Reduced Variability**: Larger k values tend to reduce the variance in the estimate of model performance because each fold is more representative of the data as a whole.\n",
        "\n",
        "### 2. **Effect of Fewer Iterations (Lower k in k-Fold Cross-Validation):**\n",
        "   - **Higher Variability**: Fewer iterations result in fewer different splits of the data, which can lead to higher variance in the performance estimate. A single fold might not be as representative of the entire dataset, leading to less reliable performance estimates.\n",
        "   - **Quicker Computation**: Fewer folds mean fewer models are trained, which can be computationally less expensive. However, this might come at the cost of less accurate performance estimation.\n",
        "\n",
        "### 3. **Trade-offs:**\n",
        "   - **Increased Computation**: More iterations (e.g., a higher k value) mean that the model has to be trained and validated more times, which increases computational cost and time.\n",
        "   - **Diminishing Returns**: After a certain point, increasing the number of folds provides diminishing returns in terms of improving the estimate. For instance, 10-fold cross-validation (k=10) is a common choice as it balances the need for a stable estimate with computational efficiency. Going beyond this often does not significantly improve the estimate and may just add computational overhead.\n",
        "\n",
        "### **Summary:**\n",
        "- **Higher k** (more iterations) generally provides a better estimate of model performance by reducing variance and providing a more stable and accurate performance measure.\n",
        "- **Lower k** (fewer iterations) can result in higher variability and less reliable estimates, but it is computationally less intensive.\n",
        "\n",
        "In practice, **k=10** is commonly used as it offers a good balance between computational efficiency and estimate reliability. However, the choice of k should consider the size of the dataset and computational resources available.\n",
        "\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "\n",
        "Answer4.Increasing the number of iterations (or folds) in cross-validation does not directly address the issues associated with having a very small training or validation dataset. Here’s how it impacts the situation:\n",
        "\n",
        "### **Impacts of Increasing Iterations in Small Datasets:**\n",
        "\n",
        "1. **More Folds**: In k-fold cross-validation, increasing the number of folds (e.g., from 5-fold to 10-fold) means that each fold will be smaller. Consequently, each training set will be larger, but each validation set will be smaller. For very small datasets:\n",
        "   - **Small Validation Sets**: With more folds, each validation set becomes smaller, which might lead to high variability in performance estimates. Small validation sets may not be representative of the overall data distribution, potentially leading to less reliable validation scores.\n",
        "   - **Training Sets**: Although each training set will be larger, the benefits might be limited by the fact that the validation set size is quite small.\n",
        "\n",
        "2. **Cross-Validation Efficiency**: While increasing the number of folds can improve the estimate of the model’s performance by averaging over multiple splits, it does not fundamentally solve the problem of limited data:\n",
        "   - **High Variability**: Smaller folds increase the variability of the validation performance estimate due to the reduced size of each validation set.\n",
        "   - **Computational Cost**: Increasing the number of folds also increases computational cost, as more models need to be trained and validated.\n",
        "\n",
        "### **Better Approaches for Small Datasets:**\n",
        "\n",
        "1. **Data Augmentation**: If possible, augmenting the dataset by generating additional synthetic data or using techniques such as oversampling can help increase the effective size of the training set.\n",
        "\n",
        "2. **Resampling Techniques**: Techniques such as bootstrapping can help create multiple datasets from the existing small dataset, providing additional data variations for training and validation.\n",
        "\n",
        "3. **Leave-One-Out Cross-Validation (LOOCV)**: For extremely small datasets, LOOCV (where each fold consists of a single data point as the validation set and the rest as the training set) can be used. However, this method can be computationally expensive and may still suffer from high variance if the dataset is very small.\n",
        "\n",
        "4. **Regularization and Simplified Models**: Using regularization techniques and simpler models can help prevent overfitting when training on a small dataset. This is because simpler models are less likely to overfit the limited data.\n",
        "\n",
        "### **Summary:**\n",
        "- Increasing the number of iterations in cross-validation does not directly address the fundamental issue of having a very small dataset. It mainly helps in improving the stability of the performance estimate.\n",
        "- For very small datasets, alternative approaches like data augmentation, bootstrapping, or using LOOCV might be more effective in addressing the challenges posed by the limited amount of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier.\n",
        "\n",
        "Answer.To understand how the accuracy of the 3-nearest neighbor (3-NN) classifier changes with the number of splits and how it is affected by the split size, and to compare it with the 1-nearest neighbor (1-NN) classifier, consider the following points:\n",
        "\n",
        "### **Impact of Number of Splits (k in k-Fold Cross-Validation):**\n",
        "\n",
        "1. **3-NN Classifier:**\n",
        "   - **More Splits (Higher k):** As the number of folds (k) increases, each fold becomes smaller. The model is trained on a larger portion of the data, but each validation set is smaller. For the 3-NN classifier, smaller validation sets may lead to higher variability in accuracy estimates because each validation fold may not fully represent the data distribution. However, the overall estimate becomes more stable as k increases and more folds are used, averaging out the variations from different folds.\n",
        "   - **Fewer Splits (Lower k):** With fewer folds, each fold is larger, providing a more representative validation set and potentially more stable accuracy estimates. However, the training set in each fold is smaller, which might affect the performance of the 3-NN classifier as it relies on the majority vote from neighbors.\n",
        "\n",
        "2. **1-NN Classifier:**\n",
        "   - **More Splits:** The 1-NN classifier might exhibit higher sensitivity to the smaller validation sets in higher k scenarios, as it is highly influenced by individual data points. Small validation sets might introduce higher variability in the accuracy estimates.\n",
        "   - **Fewer Splits:** With larger validation sets (when k is smaller), the performance estimates might be more stable, but the training sets are smaller, which could affect the performance of the 1-NN classifier since it relies on precise nearest neighbors.\n",
        "\n",
        "### **Impact of Split Size:**\n",
        "\n",
        "1. **3-NN Classifier:**\n",
        "   - **Smaller Splits (Higher k):** Each training set is larger, which can help in capturing more general patterns in the data. Smaller validation sets may lead to more variable estimates of accuracy.\n",
        "   - **Larger Splits (Lower k):** Larger validation sets might provide more reliable performance estimates. However, the training set is smaller, which could impact the ability of the 3-NN classifier to learn effectively, as it relies on multiple neighbors for classification.\n",
        "\n",
        "2. **1-NN Classifier:**\n",
        "   - **Smaller Splits:** Smaller validation sets might lead to higher variability in accuracy due to the sensitivity of 1-NN to individual data points.\n",
        "   - **Larger Splits:** Larger validation sets might yield more stable accuracy estimates. However, with smaller training sets, the 1-NN classifier might struggle due to the reduced amount of data available to determine nearest neighbors.\n",
        "\n",
        "### **Comparison:**\n",
        "\n",
        "- **Accuracy Stability:** Generally, both 1-NN and 3-NN classifiers will see more stable accuracy estimates with fewer splits (larger validation sets) as the validation sets are more representative of the overall data. However, 3-NN might be less sensitive to small validation sets compared to 1-NN, due to its reliance on multiple neighbors for classification.\n",
        "  \n",
        "- **Performance Sensitivity:**\n",
        "  - **1-NN**: Highly sensitive to individual data points; small validation sets can lead to significant variability in accuracy estimates.\n",
        "  - **3-NN**: Less sensitive to individual data points and might perform better with smaller validation sets compared to 1-NN due to its averaging effect.\n",
        "\n",
        "### **Summary:**\n",
        "- **3-NN Classifier:** Accuracy estimates become more stable with higher k (more splits) as the model benefits from being trained on larger portions of the data. However, the accuracy may vary more with smaller validation sets.\n",
        "- **1-NN Classifier:** Accuracy estimates can be highly variable with smaller validation sets due to its sensitivity to individual points. Larger validation sets generally provide more stable estimates.\n",
        "\n",
        "The choice of the number of splits should balance computational efficiency with the need for reliable performance estimates. Typically, **10-fold cross-validation** offers a good trade-off between stability and computational cost for most models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}